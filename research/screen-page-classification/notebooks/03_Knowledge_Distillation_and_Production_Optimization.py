#!/usr/bin/env python3
"""
Screen Page Classification Pipeline - Knowledge Distillation & Production Optimization

This notebook focuses on knowledge distillation and production optimization for deployment.

Objectives:
- Implement knowledge distillation from teacher to student model
- Compare teacher vs student model performance
- Optimize models for production deployment
- Analyze inference speed and memory usage
- Create production-ready lightweight models
"""

# Import required libraries
import sys
import os
import json
import warnings
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import time
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn

# Add the parent directory to the path to import our modules
sys.path.append(str(Path.cwd().parent))

from data_loader import DatasetConfig, create_data_loaders
from models import ModelFactory, get_model_info, DistillationLoss
from trainer import DistillationTrainer, get_device, MetricsTracker

# Configure plotting
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

console = Console()

def main():
    print("üéì Knowledge Distillation and Production Optimization")
    print("=" * 60)
    
    # Load prepared data
    try:
        import pickle
        with open('prepared_data.pkl', 'rb') as f:
            data_info = pickle.load(f)
        
        train_data = data_info['train_data']
        val_data = data_info['val_data']
        test_data = data_info['test_data']
        class_names = data_info['class_names']
        class_to_idx = data_info['class_to_idx']
        idx_to_class = data_info['idx_to_class']
        
        print(f"‚úÖ Loaded prepared data: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test")
        
    except FileNotFoundError:
        print("‚ùå Prepared data not found. Please run notebook 1 first.")
        return
    
    # Load best teacher model
    teacher_model_path = find_best_teacher_model()
    if not teacher_model_path:
        print("‚ùå No teacher model found. Please run notebook 2 first.")
        return
    
    print(f"üìö Teacher model: {teacher_model_path}")
    
    # Initialize data configuration
    config = DatasetConfig()
    device = get_device()
    
    # Create data loaders
    train_loader, val_loader, test_loader = create_data_loaders(
        train_data, config.data_root, class_to_idx, config, test_size=0.0, val_size=0.0
    )
    
    val_loader, _ = create_data_loaders(
        val_data, config.data_root, class_to_idx, config, test_size=0.0, val_size=0.0
    )
    _, test_loader = create_data_loaders(
        test_data, config.data_root, class_to_idx, config, test_size=0.0, val_size=0.0
    )
    
    # Load teacher model
    teacher_model = load_teacher_model(teacher_model_path, class_names, device)
    
    # Run knowledge distillation experiments
    distillation_results = run_distillation_experiments(
        teacher_model, train_loader, val_loader, test_loader, 
        class_names, device, config
    )
    
    # Analyze distillation results
    analyze_distillation_results(distillation_results, teacher_model, class_names, device)
    
    # Production optimization
    production_analysis = optimize_for_production(distillation_results, test_loader, class_names, device)
    
    # Create deployment recommendations
    create_deployment_recommendations(distillation_results, production_analysis)
    
    print("\n‚úÖ Knowledge distillation and production optimization complete!")

def find_best_teacher_model():
    """Find the best teacher model from previous experiments."""
    model_files = list(Path('.').glob('best_model_*.pth'))
    if not model_files:
        return None
    
    # For demo, return the first available model
    return str(model_files[0])

def load_teacher_model(model_path, class_names, device):
    """Load the teacher model from checkpoint."""
    checkpoint = torch.load(model_path, map_location=device)
    
    # Get model type from filename
    model_type = Path(model_path).stem.replace('best_model_', '')
    
    # Create model
    model = ModelFactory.create_model(model_type, num_classes=len(class_names))
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    console.print(f"[green]‚úÖ Teacher model loaded: {model_type}[/green]")
    console.print(f"Parameters: {checkpoint['model_info']['parameters']['total_parameters']:,}")
    console.print(f"Size: {checkpoint['model_info']['model_size_mb']:.2f} MB")
    
    return model

def run_distillation_experiments(teacher_model, train_loader, val_loader, test_loader, 
                                class_names, device, config):
    """Run knowledge distillation experiments with different configurations."""
    
    # Define distillation configurations
    distillation_configs = [
        {'temperature': 3.0, 'alpha': 0.7, 'name': 'Standard'},
        {'temperature': 5.0, 'alpha': 0.8, 'name': 'High Temp'},
        {'temperature': 2.0, 'alpha': 0.6, 'name': 'Low Temp'},
        {'temperature': 4.0, 'alpha': 0.9, 'name': 'High Alpha'}
    ]
    
    results = {}
    
    for config_dict in distillation_configs:
        config_name = config_dict['name']
        console.print(f"\n[bold blue]Running distillation: {config_name}[/bold blue]")
        
        try:
            # Create student model
            student_model = ModelFactory.create_model('lightweight', num_classes=len(class_names))
            student_model = student_model.to(device)
            
            # Create distillation trainer
            trainer = DistillationTrainer(
                model=student_model,
                config=config,
                experiment_name=f"distillation_{config_name.lower().replace(' ', '_')}",
                use_wandb=False,
                use_tensorboard=False
            )
            
            # Set distillation parameters
            trainer.distillation_loss = DistillationLoss(
                temperature=config_dict['temperature'],
                alpha=config_dict['alpha']
            )
            
            # Train with distillation
            start_time = time.time()
            distillation_results = trainer.train_with_distillation(
                train_loader=train_loader,
                val_loader=val_loader,
                teacher_model=teacher_model,
                num_epochs=15,  # Reduced for demo
                learning_rate=1e-4,
                device=device
            )
            training_time = time.time() - start_time
            
            # Evaluate student model
            student_test_results = trainer.evaluate(test_loader, device)
            
            # Store results
            results[config_name] = {
                'config': config_dict,
                'student_model': student_model,
                'trainer': trainer,
                'distillation_results': distillation_results,
                'test_results': student_test_results,
                'training_time': training_time
            }
            
            console.print(f"[green]‚úÖ {config_name} completed! Student F1: {student_test_results['test_metrics']['f1_score']:.4f}[/green]")
            
        except Exception as e:
            console.print(f"[red]‚ùå Error in {config_name}: {e}[/red]")
            continue
    
    return results

def analyze_distillation_results(distillation_results, teacher_model, class_names, device):
    """Analyze and visualize knowledge distillation results."""
    
    if not distillation_results:
        print("‚ùå No distillation results to analyze")
        return
    
    # Evaluate teacher model for comparison
    teacher_info = get_model_info(teacher_model)
    
    # Create comparison visualizations
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # Extract metrics
    config_names = list(distillation_results.keys())
    student_f1s = [distillation_results[name]['test_results']['test_metrics']['f1_score'] for name in config_names]
    student_accs = [distillation_results[name]['test_results']['test_metrics']['accuracy'] for name in config_names]
    student_sizes = [get_model_info(distillation_results[name]['student_model'])['model_size_mb'] for name in config_names]
    training_times = [distillation_results[name]['training_time'] for name in config_names]
    
    # 1. Teacher vs Student F1 Scores
    teacher_f1 = 0.85  # Mock teacher F1 score
    x_pos = np.arange(len(config_names) + 1)
    f1_scores = [teacher_f1] + student_f1s
    labels = ['Teacher'] + config_names
    
    bars1 = axes[0, 0].bar(x_pos, f1_scores, color=['red'] + ['skyblue'] * len(config_names), alpha=0.7)
    axes[0, 0].set_title('Teacher vs Student F1 Scores', fontsize=14, fontweight='bold')
    axes[0, 0].set_ylabel('F1 Score')
    axes[0, 0].set_xticks(x_pos)
    axes[0, 0].set_xticklabels(labels, rotation=45)
    
    for bar, score in zip(bars1, f1_scores):
        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                       f'{score:.3f}', ha='center', va='bottom')
    
    # 2. Model Size Comparison
    teacher_size = teacher_info['model_size_mb']
    sizes = [teacher_size] + student_sizes
    
    bars2 = axes[0, 1].bar(x_pos, sizes, color=['red'] + ['lightcoral'] * len(config_names), alpha=0.7)
    axes[0, 1].set_title('Model Size Comparison', fontsize=14, fontweight='bold')
    axes[0, 1].set_ylabel('Model Size (MB)')
    axes[0, 1].set_xticks(x_pos)
    axes[0, 1].set_xticklabels(labels, rotation=45)
    
    for bar, size in zip(bars2, sizes):
        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                       f'{size:.1f}MB', ha='center', va='bottom')
    
    # 3. Performance vs Size Trade-off
    axes[0, 2].scatter(student_sizes, student_f1s, s=100, alpha=0.7, c=range(len(config_names)), cmap='viridis')
    axes[0, 2].scatter([teacher_size], [teacher_f1], s=150, c='red', marker='*', label='Teacher')
    for i, name in enumerate(config_names):
        axes[0, 2].annotate(name, (student_sizes[i], student_f1s[i]), xytext=(5, 5), textcoords='offset points')
    axes[0, 2].set_xlabel('Model Size (MB)')
    axes[0, 2].set_ylabel('F1 Score')
    axes[0, 2].set_title('Performance vs Size Trade-off', fontsize=14, fontweight='bold')
    axes[0, 2].legend()
    
    # 4. Training Time Comparison
    bars4 = axes[1, 0].bar(config_names, training_times, color='lightgreen', alpha=0.7)
    axes[1, 0].set_title('Distillation Training Time', fontsize=14, fontweight='bold')
    axes[1, 0].set_ylabel('Training Time (seconds)')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    for bar, time in zip(bars4, training_times):
        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                       f'{time:.1f}s', ha='center', va='bottom')
    
    # 5. Knowledge Transfer Efficiency
    transfer_efficiency = [(f1 / teacher_f1) * 100 for f1 in student_f1s]
    bars5 = axes[1, 1].bar(config_names, transfer_efficiency, color='gold', alpha=0.7)
    axes[1, 1].set_title('Knowledge Transfer Efficiency', fontsize=14, fontweight='bold')
    axes[1, 1].set_ylabel('Efficiency (%)')
    axes[1, 1].tick_params(axis='x', rotation=45)
    axes[1, 1].axhline(y=90, color='red', linestyle='--', alpha=0.7, label='90% threshold')
    axes[1, 1].legend()
    
    for bar, eff in zip(bars5, transfer_efficiency):
        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                       f'{eff:.1f}%', ha='center', va='bottom')
    
    # 6. Compression Ratio
    compression_ratios = [teacher_size / size for size in student_sizes]
    bars6 = axes[1, 2].bar(config_names, compression_ratios, color='purple', alpha=0.7)
    axes[1, 2].set_title('Model Compression Ratio', fontsize=14, fontweight='bold')
    axes[1, 2].set_ylabel('Compression Ratio')
    axes[1, 2].tick_params(axis='x', rotation=45)
    
    for bar, ratio in zip(bars6, compression_ratios):
        axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                       f'{ratio:.1f}x', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('distillation_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Create detailed comparison table
    create_distillation_table(distillation_results, teacher_info)

def create_distillation_table(distillation_results, teacher_info):
    """Create a detailed comparison table of distillation results."""
    
    table = Table(title="Knowledge Distillation Results")
    table.add_column("Configuration", style="cyan")
    table.add_column("Student F1", style="green")
    table.add_column("Student Acc", style="blue")
    table.add_column("Model Size (MB)", style="magenta")
    table.add_column("Transfer Eff (%)", style="yellow")
    table.add_column("Compression", style="red")
    
    teacher_f1 = 0.85  # Mock teacher F1
    teacher_size = teacher_info['model_size_mb']
    
    for config_name, result in distillation_results.items():
        student_f1 = result['test_results']['test_metrics']['f1_score']
        student_acc = result['test_results']['test_metrics']['accuracy']
        student_size = get_model_info(result['student_model'])['model_size_mb']
        transfer_eff = (student_f1 / teacher_f1) * 100
        compression = teacher_size / student_size
        
        table.add_row(
            config_name,
            f"{student_f1:.4f}",
            f"{student_acc:.4f}",
            f"{student_size:.2f}",
            f"{transfer_eff:.1f}",
            f"{compression:.1f}x"
        )
    
    console.print(table)

def optimize_for_production(distillation_results, test_loader, class_names, device):
    """Optimize models for production deployment."""
    
    if not distillation_results:
        print("‚ùå No distillation results to optimize")
        return {}
    
    # Find best student model
    best_config = max(distillation_results.keys(), 
                     key=lambda x: distillation_results[x]['test_results']['test_metrics']['f1_score'])
    best_student = distillation_results[best_config]['student_model']
    
    console.print(f"\n[bold green]Optimizing best student model: {best_config}[/bold green]")
    
    # Performance analysis
    performance_analysis = analyze_inference_performance(best_student, test_loader, device)
    
    # Memory analysis
    memory_analysis = analyze_memory_usage(best_student, device)
    
    # Quantization analysis
    quantization_analysis = analyze_quantization(best_student, test_loader, device)
    
    # Create production optimization report
    optimization_report = {
        'best_config': best_config,
        'performance': performance_analysis,
        'memory': memory_analysis,
        'quantization': quantization_analysis
    }
    
    return optimization_report

def analyze_inference_performance(model, test_loader, device):
    """Analyze inference performance of the model."""
    
    model.eval()
    model = model.to(device)
    
    # Warm up
    dummy_input = torch.randn(1, 3, 224, 224).to(device)
    for _ in range(10):
        with torch.no_grad():
            _ = model(dummy_input)
    
    # Measure inference time
    times = []
    with torch.no_grad():
        for data, _ in test_loader:
            data = data.to(device)
            start_time = time.time()
            _ = model(data)
            end_time = time.time()
            times.append((end_time - start_time) / data.size(0))  # Per sample
    
    avg_inference_time = np.mean(times) * 1000  # Convert to ms
    std_inference_time = np.std(times) * 1000
    
    # Calculate throughput
    batch_size = test_loader.batch_size
    throughput = batch_size / (avg_inference_time / 1000)  # Samples per second
    
    console.print(f"‚ö° Inference Performance:")
    console.print(f"  Average time per sample: {avg_inference_time:.2f} ¬± {std_inference_time:.2f} ms")
    console.print(f"  Throughput: {throughput:.1f} samples/second")
    
    return {
        'avg_inference_time_ms': avg_inference_time,
        'std_inference_time_ms': std_inference_time,
        'throughput_sps': throughput
    }

def analyze_memory_usage(model, device):
    """Analyze memory usage of the model."""
    
    # Model size
    model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)  # MB
    
    # Memory footprint during inference
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        dummy_input = torch.randn(1, 3, 224, 224).to(device)
        with torch.no_grad():
            _ = model(dummy_input)
        
        peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024)  # MB
        current_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # MB
    else:
        peak_memory = model_size
        current_memory = model_size
    
    console.print(f"üíæ Memory Usage:")
    console.print(f"  Model size: {model_size:.2f} MB")
    console.print(f"  Peak memory: {peak_memory:.2f} MB")
    console.print(f"  Current memory: {current_memory:.2f} MB")
    
    return {
        'model_size_mb': model_size,
        'peak_memory_mb': peak_memory,
        'current_memory_mb': current_memory
    }

def analyze_quantization(model, test_loader, device):
    """Analyze the effect of quantization on model performance."""
    
    # Create quantized model
    quantized_model = torch.quantization.quantize_dynamic(
        model, {nn.Linear, nn.Conv2d}, dtype=torch.qint8
    )
    
    # Evaluate quantized model
    quantized_model.eval()
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = quantized_model(data)
            predictions = torch.argmax(output, dim=1)
            
            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # Calculate accuracy
    quantized_accuracy = accuracy_score(all_targets, all_predictions)
    
    # Calculate model size reduction
    original_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)
    quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters()) / (1024 * 1024)
    size_reduction = (1 - quantized_size / original_size) * 100
    
    console.print(f"üîß Quantization Analysis:")
    console.print(f"  Quantized accuracy: {quantized_accuracy:.4f}")
    console.print(f"  Size reduction: {size_reduction:.1f}%")
    console.print(f"  Original size: {original_size:.2f} MB")
    console.print(f"  Quantized size: {quantized_size:.2f} MB")
    
    return {
        'quantized_accuracy': quantized_accuracy,
        'size_reduction_percent': size_reduction,
        'original_size_mb': original_size,
        'quantized_size_mb': quantized_size
    }

def create_deployment_recommendations(distillation_results, production_analysis):
    """Create deployment recommendations based on analysis."""
    
    console.print(f"\n[bold blue]üöÄ Production Deployment Recommendations[/bold blue]")
    console.print("=" * 60)
    
    if not distillation_results or not production_analysis:
        console.print("‚ùå No analysis data available for recommendations")
        return
    
    # Find best configuration
    best_config = production_analysis['best_config']
    best_result = distillation_results[best_config]
    
    # Get performance metrics
    perf = production_analysis['performance']
    memory = production_analysis['memory']
    quant = production_analysis['quantization']
    
    # Create recommendations
    recommendations = []
    
    # Model selection
    recommendations.append(f"‚úÖ **Best Model**: {best_config} configuration")
    recommendations.append(f"   - F1 Score: {best_result['test_results']['test_metrics']['f1_score']:.4f}")
    recommendations.append(f"   - Model Size: {memory['model_size_mb']:.2f} MB")
    recommendations.append(f"   - Inference Time: {perf['avg_inference_time_ms']:.2f} ms")
    
    # Deployment scenarios
    recommendations.append(f"\nüì± **Mobile Deployment**:")
    recommendations.append(f"   - Use quantized model for 50% size reduction")
    recommendations.append(f"   - Expected accuracy: {quant['quantized_accuracy']:.4f}")
    recommendations.append(f"   - Final size: {quant['quantized_size_mb']:.2f} MB")
    
    recommendations.append(f"\n‚òÅÔ∏è **Cloud Deployment**:")
    recommendations.append(f"   - Use full precision model for maximum accuracy")
    recommendations.append(f"   - Throughput: {perf['throughput_sps']:.1f} samples/second")
    recommendations.append(f"   - Memory requirement: {memory['peak_memory_mb']:.2f} MB")
    
    recommendations.append(f"\nüîß **Optimization Tips**:")
    recommendations.append(f"   - Use TensorRT for GPU acceleration")
    recommendations.append(f"   - Implement model caching for repeated predictions")
    recommendations.append(f"   - Consider batch processing for higher throughput")
    
    # Print recommendations
    for rec in recommendations:
        console.print(rec)
    
    # Save recommendations to file
    with open('deployment_recommendations.md', 'w') as f:
        f.write("# Production Deployment Recommendations\n\n")
        for rec in recommendations:
            f.write(rec + "\n")
    
    console.print(f"\nüíæ Recommendations saved to: deployment_recommendations.md")

if __name__ == "__main__":
    main()